---
name: scientific-paper-quality
description: |
  è«–æ–‡å“è³ªã®å®šé‡çš„è©•ä¾¡ã‚¹ã‚­ãƒ«ã€‚å¯èª­æ€§ã‚¹ã‚³ã‚¢ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒ©ãƒ³ã‚¹åˆ†æã€
  èªå½™å¤šæ§˜æ€§ã€å­¦è¡“èªä½¿ç”¨ç‡ã€å†—é•·è¡¨ç¾æ¤œå‡ºã€ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«è¦ä»¶é©åˆãƒã‚§ãƒƒã‚¯ã€
  å†ç¾å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
  ã€Œè«–æ–‡ã®å“è³ªã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã€ã€Œå¯èª­æ€§ã‚¹ã‚³ã‚¢ã‚’å‡ºã—ã¦ã€ã€ŒæŠ•ç¨¿å‰ãƒã‚§ãƒƒã‚¯ã€ã§ç™ºç«ã€‚
---

# Scientific Paper Quality

è«–æ–‡å“è³ªã‚’å®šé‡çš„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§è©•ä¾¡ã—ã€æŠ•ç¨¿å‰ã®å“è³ªä¿è¨¼ã‚’æ”¯æ´ã™ã‚‹ã‚¹ã‚­ãƒ«ã€‚
å¯èª­æ€§ãƒ»æ§‹é€ ãƒ»èªå½™ãƒ»ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«é©åˆæ€§ã‚’å¤šè§’çš„ã«ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã™ã‚‹ã€‚

## When to Use

- æŠ•ç¨¿å‰ã®æœ€çµ‚å“è³ªãƒã‚§ãƒƒã‚¯ã‚’è¡Œã†ã¨ã
- åŸç¨¿ã®å¯èª­æ€§ã‚’å®šé‡çš„ã«è©•ä¾¡ã—ãŸã„ã¨ã
- ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«æŠ•ç¨¿è¦ä»¶ï¼ˆèªæ•°åˆ¶é™ã€å›³è¡¨æ•°ç­‰ï¼‰ã¸ã®é©åˆã‚’ç¢ºèªã™ã‚‹ã¨ã
- ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–“ã®ãƒãƒ©ãƒ³ã‚¹ã‚’è©•ä¾¡ã—ãŸã„ã¨ã
- å†—é•·è¡¨ç¾ãƒ»å¼±ã„å‹•è©ãƒ»éå‰°ä¸»å¼µã‚’æ¤œå‡ºã—ãŸã„ã¨ã
- æ”¹è¨‚å‰å¾Œã®å“è³ªå¤‰åŒ–ã‚’æ¯”è¼ƒã—ãŸã„ã¨ã
- å†ç¾å¯èƒ½æ€§ã®è¦³ç‚¹ã‹ã‚‰ Methods ã‚’æ¤œè¨¼ã—ãŸã„ã¨ã

## Quick Start

## 1. å“è³ªãƒã‚§ãƒƒã‚¯ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

```
åŸç¨¿ (manuscript/manuscript.md)
  â”œâ”€ Dimension 1: å¯èª­æ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹
  â”‚   â”œâ”€ Flesch-Kincaid Grade Level
  â”‚   â”œâ”€ Gunning Fog Index
  â”‚   â”œâ”€ å¹³å‡æ–‡é•· / å¹³å‡å˜èªé•·
  â”‚   â””â”€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥å¯èª­æ€§
  â”œâ”€ Dimension 2: æ§‹é€ å“è³ª
  â”‚   â”œâ”€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³é•·ãƒãƒ©ãƒ³ã‚¹ï¼ˆIMRAD æ¯”ç‡ï¼‰
  â”‚   â”œâ”€ æ®µè½æ§‹æˆã®é©åˆ‡ã•
  â”‚   â”œâ”€ å›³è¡¨-ãƒ†ã‚­ã‚¹ãƒˆå‚ç…§ã®ç¶²ç¾…æ€§
  â”‚   â””â”€ ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–“ã®è«–ç†ãƒ•ãƒ­ãƒ¼
  â”œâ”€ Dimension 3: èªå½™ãƒ»è¡¨ç¾å“è³ª
  â”‚   â”œâ”€ èªå½™å¤šæ§˜æ€§ (TTR / MTLD)
  â”‚   â”œâ”€ å­¦è¡“èªä½¿ç”¨ç‡
  â”‚   â”œâ”€ å†—é•·è¡¨ç¾ã®æ¤œå‡º
  â”‚   â”œâ”€ å¼±ã„å‹•è© / æ›–æ˜§è¡¨ç¾ã®æ¤œå‡º
  â”‚   â””â”€ ãƒ˜ãƒƒã‚¸è¡¨ç¾ã®é©åˆ‡æ€§
  â”œâ”€ Dimension 4: ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«é©åˆæ€§
  â”‚   â”œâ”€ èªæ•°åˆ¶é™ãƒã‚§ãƒƒã‚¯
  â”‚   â”œâ”€ å›³è¡¨æ•°åˆ¶é™ãƒã‚§ãƒƒã‚¯
  â”‚   â”œâ”€ å‚è€ƒæ–‡çŒ®æ•°ãƒã‚§ãƒƒã‚¯
  â”‚   â””â”€ ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè¦ä»¶é©åˆ
  â”œâ”€ Dimension 5: å†ç¾å¯èƒ½æ€§
  â”‚   â”œâ”€ Methods ã®è©³ç´°åº¦
  â”‚   â”œâ”€ çµ±è¨ˆæ‰‹æ³•ã®è¨˜è¼‰å®Œå…¨æ€§
  â”‚   â”œâ”€ ãƒ‡ãƒ¼ã‚¿å¯ç”¨æ€§è¨˜è¼‰
  â”‚   â””â”€ ã‚³ãƒ¼ãƒ‰/ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ãƒãƒ¼ã‚¸ãƒ§ãƒ³è¨˜è¼‰
  â””â”€ ç·åˆã‚¹ã‚³ã‚¢ã‚«ãƒ¼ãƒ‰å‡ºåŠ›
      â””â”€ manuscript/quality_report.json
```

## 2. å¯èª­æ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹

```python
import re
import json
import math
from pathlib import Path
from collections import Counter


def compute_readability(text):
    """
    ãƒ†ã‚­ã‚¹ãƒˆã®å¯èª­æ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ã™ã‚‹ã€‚

    Args:
        text: str â€” è§£æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ

    Returns:
        dict: {
            "flesch_kincaid_grade": float,
            "gunning_fog": float,
            "avg_sentence_length": float,
            "avg_word_length": float,
            "sentences": int,
            "words": int,
            "syllables": int,
            "complex_words": int,
        }
    """
    # å‰å‡¦ç†: Markdown æ§‹æ–‡ã‚’é™¤å»
    clean = _strip_markdown(text)

    sentences = _split_sentences(clean)
    words = _tokenize_words(clean)
    n_sentences = len(sentences)
    n_words = len(words)

    if n_sentences == 0 or n_words == 0:
        return {"error": "ãƒ†ã‚­ã‚¹ãƒˆãŒçŸ­ã™ãã¾ã™"}

    syllable_counts = [_count_syllables(w) for w in words]
    n_syllables = sum(syllable_counts)
    complex_words = sum(1 for s in syllable_counts if s >= 3)

    # Flesch-Kincaid Grade Level
    fk_grade = (0.39 * (n_words / n_sentences)
                + 11.8 * (n_syllables / n_words)
                - 15.59)

    # Gunning Fog Index
    fog = 0.4 * ((n_words / n_sentences) + 100 * (complex_words / n_words))

    return {
        "flesch_kincaid_grade": round(fk_grade, 1),
        "gunning_fog": round(fog, 1),
        "avg_sentence_length": round(n_words / n_sentences, 1),
        "avg_word_length": round(sum(len(w) for w in words) / n_words, 1),
        "sentences": n_sentences,
        "words": n_words,
        "syllables": n_syllables,
        "complex_words": complex_words,
    }


def _strip_markdown(text):
    """Markdown æ§‹æ–‡ã‚’é™¤å»ã—ã¦ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ã™ã‚‹ã€‚"""
    text = re.sub(r'^#{1,6}\s+', '', text, flags=re.MULTILINE)
    text = re.sub(r'\*\*(.+?)\*\*', r'\1', text)
    text = re.sub(r'\*(.+?)\*', r'\1', text)
    text = re.sub(r'!\[.*?\]\(.*?\)', '', text)
    text = re.sub(r'\[(.+?)\]\(.*?\)', r'\1', text)
    text = re.sub(r'```.*?```', '', text, flags=re.DOTALL)
    text = re.sub(r'`(.+?)`', r'\1', text)
    text = re.sub(r'^\|.*\|$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*[-*]\s+', '', text, flags=re.MULTILINE)
    return text


def _split_sentences(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ–‡ã«åˆ†å‰²ã™ã‚‹ã€‚"""
    sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z])', text)
    return [s.strip() for s in sentences if s.strip() and len(s.strip()) > 5]


def _tokenize_words(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’å˜èªã«åˆ†å‰²ã™ã‚‹ã€‚"""
    return re.findall(r'\b[a-zA-Z]+\b', text)


def _count_syllables(word):
    """å˜èªã®éŸ³ç¯€æ•°ã‚’æ¨å®šã™ã‚‹ã€‚"""
    word = word.lower()
    if len(word) <= 3:
        return 1
    vowels = "aeiou"
    count = 0
    prev_vowel = False
    for char in word:
        is_vowel = char in vowels
        if is_vowel and not prev_vowel:
            count += 1
        prev_vowel = is_vowel
    if word.endswith('e') and count > 1:
        count -= 1
    return max(1, count)
```

## 3. æ§‹é€ å“è³ªåˆ†æ

```python
IMRAD_BALANCE = {
    "ideal_ratios": {
        "Introduction": (0.15, 0.25),
        "Methods": (0.15, 0.30),
        "Results": (0.20, 0.35),
        "Discussion": (0.20, 0.35),
    },
    "abstract_max_words": {
        "nature": 150,
        "science": 125,
        "acs": 250,
        "ieee": 250,
        "elsevier": 300,
        "default": 250,
    },
}


def analyze_structure(text):
    """
    è«–æ–‡æ§‹é€ ã®å“è³ªã‚’åˆ†æã™ã‚‹ã€‚

    Returns:
        dict: {
            "section_word_counts": {"Introduction": 450, ...},
            "section_ratios": {"Introduction": 0.18, ...},
            "balance_score": float (0-1),
            "balance_issues": [...],
            "figure_text_coverage": {...},
            "paragraph_stats": {...},
        }
    """
    sections = _split_into_sections(text)
    total_body_words = 0
    section_words = {}

    # IMRAD ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®èªæ•°
    for name, content in sections.items():
        clean = _strip_markdown(content)
        wc = len(_tokenize_words(clean))
        section_words[name] = wc
        if any(k.lower() in name.lower() for k in
               ["introduction", "method", "result", "discussion"]):
            total_body_words += wc

    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ¯”ç‡
    section_ratios = {}
    if total_body_words > 0:
        for name, wc in section_words.items():
            section_ratios[name] = round(wc / total_body_words, 3)

    # ãƒãƒ©ãƒ³ã‚¹è©•ä¾¡
    balance_issues = []
    balance_scores = []

    for section, (low, high) in IMRAD_BALANCE["ideal_ratios"].items():
        matched_key = None
        for key in section_ratios:
            if section.lower() in key.lower():
                matched_key = key
                break

        if matched_key:
            ratio = section_ratios[matched_key]
            if ratio < low:
                balance_issues.append(
                    f"{section} ãŒçŸ­ã™ãã¾ã™ ({ratio:.0%} < {low:.0%})"
                )
                balance_scores.append(ratio / low)
            elif ratio > high:
                balance_issues.append(
                    f"{section} ãŒé•·ã™ãã¾ã™ ({ratio:.0%} > {high:.0%})"
                )
                balance_scores.append(high / ratio)
            else:
                balance_scores.append(1.0)

    balance_score = sum(balance_scores) / len(balance_scores) if balance_scores else 0.5

    # å›³è¡¨å‚ç…§ãƒã‚§ãƒƒã‚¯
    figure_refs = set(re.findall(r'(?:Fig(?:ure)?\.?\s*|fig\.?\s*)(\d+)', text, re.IGNORECASE))
    figure_embeds = set(re.findall(r'!\[.*?(?:Fig(?:ure)?\.?\s*|fig\.?\s*)(\d+)', text, re.IGNORECASE))
    table_refs = set(re.findall(r'Table\s+(\d+)', text, re.IGNORECASE))

    # æ®µè½çµ±è¨ˆ
    paragraphs = [p for p in text.split('\n\n') if p.strip() and not p.strip().startswith('#')]
    para_lengths = [len(_tokenize_words(p)) for p in paragraphs]

    return {
        "section_word_counts": section_words,
        "section_ratios": section_ratios,
        "balance_score": round(balance_score, 2),
        "balance_issues": balance_issues,
        "figure_text_coverage": {
            "figures_referenced": sorted(figure_refs),
            "figures_embedded": sorted(figure_embeds),
            "tables_referenced": sorted(table_refs),
        },
        "paragraph_stats": {
            "count": len(paragraphs),
            "avg_length": round(sum(para_lengths) / max(1, len(para_lengths)), 1),
            "min_length": min(para_lengths) if para_lengths else 0,
            "max_length": max(para_lengths) if para_lengths else 0,
        },
    }
```

## 4. èªå½™ãƒ»è¡¨ç¾å“è³ª

```python
WEAK_VERBS = [
    "is", "are", "was", "were", "been", "being",
    "have", "has", "had", "do", "does", "did",
    "make", "makes", "made", "get", "gets", "got",
    "seem", "seems", "seemed", "appear", "appears",
]

REDUNDANT_PHRASES = {
    "in order to": "to",
    "due to the fact that": "because",
    "it is important to note that": "[omit]",
    "it should be noted that": "[omit]",
    "a large number of": "many",
    "a small number of": "few",
    "in the case of": "for",
    "on the other hand": "conversely",
    "at the present time": "now / currently",
    "in the event that": "if",
    "in close proximity to": "near",
    "has the ability to": "can",
    "is able to": "can",
    "as a matter of fact": "[omit]",
    "it is well known that": "[omit or cite]",
    "it has been shown that": "[cite]",
    "take into account": "consider",
    "a number of": "several",
    "the majority of": "most",
    "prior to": "before",
    "subsequent to": "after",
    "in spite of": "despite",
    "for the purpose of": "to / for",
}

HEDGE_WORDS = [
    "might", "could", "may", "possibly", "perhaps",
    "somewhat", "relatively", "apparently", "presumably",
    "likely", "unlikely", "tend to", "seems to",
]

OVERCLAIM_PHRASES = [
    "clearly demonstrates", "undoubtedly", "proves that",
    "unequivocally", "definitely", "without doubt",
    "conclusively proves", "perfect", "always",
    "never", "all cases", "completely",
    "the first to", "novel", "unprecedented",
    "for the first time",
]


def analyze_vocabulary(text):
    """
    èªå½™ãƒ»è¡¨ç¾ã®å“è³ªã‚’åˆ†æã™ã‚‹ã€‚

    Returns:
        dict: {
            "vocabulary_diversity": float (TTR),
            "academic_word_ratio": float,
            "weak_verb_count": int,
            "weak_verb_examples": [...],
            "redundant_phrases": [...],
            "hedge_count": int,
            "overclaim_count": int,
            "overclaim_examples": [...],
        }
    """
    clean = _strip_markdown(text)
    words = _tokenize_words(clean)
    words_lower = [w.lower() for w in words]

    if not words:
        return {"error": "ãƒ†ã‚­ã‚¹ãƒˆãŒçŸ­ã™ãã¾ã™"}

    # Type-Token Ratio (TTR)
    unique_words = set(words_lower)
    ttr = len(unique_words) / len(words_lower)

    # å¼±ã„å‹•è©
    weak_verb_positions = []
    for i, w in enumerate(words_lower):
        if w in WEAK_VERBS:
            context_start = max(0, i - 3)
            context_end = min(len(words), i + 4)
            weak_verb_positions.append({
                "verb": w,
                "context": " ".join(words[context_start:context_end]),
            })

    # å†—é•·è¡¨ç¾
    redundant_found = []
    text_lower = clean.lower()
    for phrase, suggestion in REDUNDANT_PHRASES.items():
        count = text_lower.count(phrase)
        if count > 0:
            redundant_found.append({
                "phrase": phrase,
                "suggestion": suggestion,
                "count": count,
            })

    # ãƒ˜ãƒƒã‚¸è¡¨ç¾
    hedge_count = sum(text_lower.count(h) for h in HEDGE_WORDS)

    # éå‰°ä¸»å¼µ
    overclaim_found = []
    for phrase in OVERCLAIM_PHRASES:
        if phrase.lower() in text_lower:
            overclaim_found.append(phrase)

    return {
        "vocabulary_diversity_ttr": round(ttr, 3),
        "total_words": len(words),
        "unique_words": len(unique_words),
        "weak_verb_count": len(weak_verb_positions),
        "weak_verb_examples": weak_verb_positions[:10],
        "redundant_phrases": redundant_found,
        "hedge_count": hedge_count,
        "overclaim_count": len(overclaim_found),
        "overclaim_examples": overclaim_found,
    }
```

## 5. ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«é©åˆæ€§ãƒã‚§ãƒƒã‚¯

```python
JOURNAL_REQUIREMENTS = {
    "nature": {
        "max_words": 3000,
        "max_figures": 8,
        "max_references": 50,
        "max_abstract_words": 150,
        "requires_data_availability": True,
        "requires_author_contributions": True,
        "requires_competing_interests": True,
    },
    "science": {
        "max_words": 2500,
        "max_figures": 4,
        "max_references": 40,
        "max_abstract_words": 125,
        "requires_data_availability": True,
        "requires_author_contributions": True,
        "requires_competing_interests": True,
    },
    "acs": {
        "max_words": 7000,
        "max_figures": 10,
        "max_references": 60,
        "max_abstract_words": 250,
        "requires_data_availability": False,
        "requires_author_contributions": True,
        "requires_competing_interests": True,
    },
    "ieee": {
        "max_words": 8000,
        "max_figures": 12,
        "max_references": 50,
        "max_abstract_words": 250,
        "requires_data_availability": False,
        "requires_author_contributions": False,
        "requires_competing_interests": False,
    },
    "elsevier": {
        "max_words": 8000,
        "max_figures": 15,
        "max_references": 80,
        "max_abstract_words": 300,
        "requires_data_availability": True,
        "requires_author_contributions": True,
        "requires_competing_interests": True,
    },
}


def check_journal_compliance(text, journal_format="elsevier"):
    """
    ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«æŠ•ç¨¿è¦ä»¶ã¸ã®é©åˆã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã€‚

    Returns:
        dict: {
            "passed": bool,
            "violations": [...],
            "warnings": [...],
            "word_count": int,
            "figure_count": int,
            "reference_count": int,
        }
    """
    reqs = JOURNAL_REQUIREMENTS.get(journal_format, JOURNAL_REQUIREMENTS["elsevier"])
    clean = _strip_markdown(text)
    words = _tokenize_words(clean)
    word_count = len(words)

    # å›³ã®æ•°
    figures = set(re.findall(r'!\[.*?\]\(.*?\)', text))
    figure_count = len(figures)

    # å‚è€ƒæ–‡çŒ®ã®æ•°
    ref_section = re.search(r'#{1,2}\s*References(.*)', text, re.DOTALL | re.IGNORECASE)
    ref_count = 0
    if ref_section:
        ref_count = len(re.findall(r'^\s*\d+[.\)]', ref_section.group(1), re.MULTILINE))

    # Abstract èªæ•°
    abstract_match = re.search(r'#{1,2}\s*Abstract\s*\n(.*?)(?=\n#{1,2}\s|\Z)',
                                text, re.DOTALL | re.IGNORECASE)
    abstract_words = len(_tokenize_words(abstract_match.group(1))) if abstract_match else 0

    violations = []
    warnings = []

    # èªæ•°ãƒã‚§ãƒƒã‚¯
    if word_count > reqs["max_words"]:
        violations.append(
            f"èªæ•°è¶…é: {word_count} / {reqs['max_words']} "
            f"(è¶…é {word_count - reqs['max_words']} èª)"
        )
    elif word_count > reqs["max_words"] * 0.9:
        warnings.append(
            f"èªæ•°ãŒä¸Šé™ã«è¿‘ã¥ã„ã¦ã„ã¾ã™: {word_count} / {reqs['max_words']}"
        )

    # å›³è¡¨æ•°ãƒã‚§ãƒƒã‚¯
    if figure_count > reqs["max_figures"]:
        violations.append(
            f"å›³è¡¨æ•°è¶…é: {figure_count} / {reqs['max_figures']}"
        )

    # å‚è€ƒæ–‡çŒ®æ•°ãƒã‚§ãƒƒã‚¯
    if ref_count > reqs["max_references"]:
        warnings.append(
            f"å‚è€ƒæ–‡çŒ®æ•°è¶…é: {ref_count} / {reqs['max_references']}"
        )

    # Abstract èªæ•°ãƒã‚§ãƒƒã‚¯
    if abstract_words > reqs["max_abstract_words"]:
        violations.append(
            f"Abstract èªæ•°è¶…é: {abstract_words} / {reqs['max_abstract_words']}"
        )

    # å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯
    text_lower = text.lower()
    if reqs.get("requires_data_availability"):
        if "data availability" not in text_lower and "data access" not in text_lower:
            violations.append("Data Availability Statement ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    if reqs.get("requires_author_contributions"):
        if "author contribution" not in text_lower and "contributions" not in text_lower:
            warnings.append("Author Contributions ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    if reqs.get("requires_competing_interests"):
        if "competing interest" not in text_lower and "conflict of interest" not in text_lower:
            warnings.append("Competing Interests å£°æ˜ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    return {
        "journal": journal_format,
        "passed": len(violations) == 0,
        "violations": violations,
        "warnings": warnings,
        "word_count": word_count,
        "abstract_words": abstract_words,
        "figure_count": figure_count,
        "reference_count": ref_count,
    }
```

## 6. å†ç¾å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯

```python
REPRODUCIBILITY_CHECKS = {
    "statistical_methods": {
        "patterns": [
            r't-test|ANOVA|chi-square|Mann-Whitney|Kruskal-Wallis|Wilcoxon',
            r'p\s*[<>=]\s*0\.\d+|Î±\s*=\s*0\.\d+|significance level',
            r'n\s*=\s*\d+|sample size|number of (?:samples|subjects|participants)',
            r'confidence interval|CI|standard (?:deviation|error)',
            r'effect size|Cohen[\'s]?\s*d|Î·Â²|RÂ²',
        ],
        "required": ["test_type", "significance_level", "sample_size"],
    },
    "software_versions": {
        "patterns": [
            r'Python\s+\d+\.\d+|R\s+\d+\.\d+|MATLAB\s+R\d{4}',
            r'(?:version|v\.?)\s*\d+\.\d+',
            r'scikit-learn|scipy|numpy|pandas|statsmodels|ggplot',
        ],
    },
    "data_availability": {
        "patterns": [
            r'data\s+(?:are|is)\s+(?:available|deposited|accessible)',
            r'(?:GitHub|Zenodo|Figshare|Dryad)',
            r'accession\s+(?:number|code)',
            r'(?:doi|DOI):\s*10\.\d+',
        ],
    },
}


def check_reproducibility(text):
    """
    Methods ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å†ç¾å¯èƒ½æ€§ã‚’è©•ä¾¡ã™ã‚‹ã€‚

    Returns:
        dict: {
            "score": float (0-1),
            "checks": {
                "statistical_methods": {"present": [...], "missing": [...]},
                "software_versions": {"present": [...], "missing": [...]},
                "data_availability": {"present": [...], "missing": [...]},
            },
            "recommendations": [...],
        }
    """
    # Methods ã‚»ã‚¯ã‚·ãƒ§ãƒ³æŠ½å‡º
    methods_match = re.search(
        r'#{1,2}\s*(?:Methods?|Materials?\s+and\s+Methods?|Experimental)\s*\n(.*?)(?=\n#{1,2}\s|\Z)',
        text, re.DOTALL | re.IGNORECASE
    )
    methods_text = methods_match.group(1) if methods_match else text

    checks = {}
    total_found = 0
    total_checks = 0

    for category, config in REPRODUCIBILITY_CHECKS.items():
        present = []
        for pattern in config["patterns"]:
            matches = re.findall(pattern, methods_text, re.IGNORECASE)
            if matches:
                present.extend(set(matches))

        checks[category] = {
            "present": present,
            "found": len(present) > 0,
        }
        total_checks += 1
        if present:
            total_found += 1

    score = total_found / total_checks if total_checks > 0 else 0

    recommendations = []
    if not checks["statistical_methods"]["found"]:
        recommendations.append(
            "çµ±è¨ˆæ‰‹æ³•ã®è©³ç´°ï¼ˆæ¤œå®šåã€æœ‰æ„æ°´æº–ã€ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºï¼‰ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„"
        )
    if not checks["software_versions"]["found"]:
        recommendations.append(
            "ä½¿ç”¨ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢/ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨ãã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„"
        )
    if not checks["data_availability"]["found"]:
        recommendations.append(
            "ãƒ‡ãƒ¼ã‚¿ã®å¯ç”¨æ€§ã«é–¢ã™ã‚‹è¨˜è¿°ï¼ˆãƒªãƒã‚¸ãƒˆãƒª URL ç­‰ï¼‰ã‚’è¿½åŠ ã—ã¦ãã ã•ã„"
        )

    return {
        "score": round(score, 2),
        "checks": checks,
        "recommendations": recommendations,
    }
```

## 7. å“è³ªã‚¹ã‚³ã‚¢ã‚«ãƒ¼ãƒ‰ãƒ»ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```python
def run_quality_check(manuscript_path, journal_format="elsevier",
                       comparison_path=None, filepath=None):
    """
    è«–æ–‡å“è³ªãƒã‚§ãƒƒã‚¯ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œã™ã‚‹ã€‚

    Args:
        manuscript_path: Path â€” åŸç¨¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        journal_format: str â€” ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«å½¢å¼
        comparison_path: Path â€” æ¯”è¼ƒç”¨åŸç¨¿ï¼ˆæ”¹è¨‚å‰ç‰ˆãªã©ã€å·®åˆ†è¡¨ç¤ºç”¨ï¼‰
        filepath: Path â€” ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›å…ˆ

    å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:
        manuscript/quality_report.json â€” å“è³ªã‚¹ã‚³ã‚¢ã‚«ãƒ¼ãƒ‰
    """
    if filepath is None:
        filepath = BASE_DIR / "manuscript" / "quality_report.json"
    filepath.parent.mkdir(parents=True, exist_ok=True)

    print("=" * 60)
    print("Paper Quality Check Pipeline")
    print("=" * 60)

    with open(manuscript_path, "r", encoding="utf-8") as f:
        text = f.read()

    # Dimension 1: å¯èª­æ€§
    print("\n[Dim 1] å¯èª­æ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ä¸­...")
    readability = compute_readability(text)
    print(f"  â†’ Flesch-Kincaid Grade: {readability.get('flesch_kincaid_grade', 'N/A')}")
    print(f"  â†’ Gunning Fog Index: {readability.get('gunning_fog', 'N/A')}")
    print(f"  â†’ å¹³å‡æ–‡é•·: {readability.get('avg_sentence_length', 'N/A')} èª")

    # Dimension 2: æ§‹é€ å“è³ª
    print("\n[Dim 2] æ§‹é€ å“è³ªã‚’åˆ†æä¸­...")
    structure = analyze_structure(text)
    print(f"  â†’ ãƒãƒ©ãƒ³ã‚¹ã‚¹ã‚³ã‚¢: {structure['balance_score']}")
    for issue in structure["balance_issues"]:
        print(f"  âš ï¸ {issue}")

    # Dimension 3: èªå½™ãƒ»è¡¨ç¾
    print("\n[Dim 3] èªå½™ãƒ»è¡¨ç¾å“è³ªã‚’åˆ†æä¸­...")
    vocabulary = analyze_vocabulary(text)
    print(f"  â†’ èªå½™å¤šæ§˜æ€§ (TTR): {vocabulary.get('vocabulary_diversity_ttr', 'N/A')}")
    print(f"  â†’ å†—é•·è¡¨ç¾: {len(vocabulary.get('redundant_phrases', []))} ç¨®")
    print(f"  â†’ éå‰°ä¸»å¼µ: {vocabulary.get('overclaim_count', 0)} ç®‡æ‰€")

    # Dimension 4: ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«é©åˆæ€§
    print(f"\n[Dim 4] ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«é©åˆæ€§ã‚’ç¢ºèªä¸­ ({journal_format})...")
    compliance = check_journal_compliance(text, journal_format)
    status = "âœ… PASS" if compliance["passed"] else "âŒ FAIL"
    print(f"  â†’ {status}")
    for v in compliance["violations"]:
        print(f"  âŒ {v}")
    for w in compliance["warnings"]:
        print(f"  âš ï¸ {w}")

    # Dimension 5: å†ç¾å¯èƒ½æ€§
    print("\n[Dim 5] å†ç¾å¯èƒ½æ€§ã‚’ç¢ºèªä¸­...")
    reproducibility = check_reproducibility(text)
    print(f"  â†’ å†ç¾å¯èƒ½æ€§ã‚¹ã‚³ã‚¢: {reproducibility['score']}")
    for rec in reproducibility["recommendations"]:
        print(f"  ğŸ’¡ {rec}")

    # ç·åˆã‚¹ã‚³ã‚¢ (0-100)
    scores = {
        "readability": _readability_score(readability),
        "structure": structure["balance_score"],
        "vocabulary": _vocabulary_score(vocabulary),
        "compliance": 1.0 if compliance["passed"] else 0.5,
        "reproducibility": reproducibility["score"],
    }
    weights = {
        "readability": 0.20,
        "structure": 0.20,
        "vocabulary": 0.20,
        "compliance": 0.25,
        "reproducibility": 0.15,
    }
    overall = sum(scores[k] * weights[k] for k in scores) * 100

    print(f"\n{'=' * 60}")
    print(f"ç·åˆå“è³ªã‚¹ã‚³ã‚¢: {overall:.0f} / 100")
    print(f"{'=' * 60}")

    # æ¯”è¼ƒï¼ˆæ”¹è¨‚å‰å¾Œï¼‰
    comparison = None
    if comparison_path:
        print("\n[æ¯”è¼ƒ] æ”¹è¨‚å‰å¾Œã®å“è³ªå¤‰åŒ–ã‚’è¨ˆç®—ä¸­...")
        with open(comparison_path, "r", encoding="utf-8") as f:
            old_text = f.read()
        old_readability = compute_readability(old_text)
        old_scores = {
            "readability": _readability_score(old_readability),
            "structure": analyze_structure(old_text)["balance_score"],
            "vocabulary": _vocabulary_score(analyze_vocabulary(old_text)),
        }
        old_overall = sum(old_scores.get(k, 0) * weights[k] for k in old_scores) * 100
        comparison = {
            "score_change": round(overall - old_overall, 1),
            "improved": overall > old_overall,
        }
        print(f"  â†’ ã‚¹ã‚³ã‚¢å¤‰åŒ–: {comparison['score_change']:+.1f}")

    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    report = {
        "manuscript": str(manuscript_path),
        "journal": journal_format,
        "overall_score": round(overall, 1),
        "dimension_scores": {k: round(v * 100, 1) for k, v in scores.items()},
        "readability": readability,
        "structure": structure,
        "vocabulary": vocabulary,
        "compliance": compliance,
        "reproducibility": reproducibility,
        "comparison": comparison,
    }

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    print(f"\n  â†’ å“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {filepath}")
    return report


def _readability_score(readability):
    """å¯èª­æ€§ã‚’ 0-1 ã‚¹ã‚³ã‚¢ã«å¤‰æ›ã™ã‚‹ã€‚å­¦è¡“è«–æ–‡ã®é©æ­£ç¯„å›²ã¯ FK Grade 12-16ã€‚"""
    fk = readability.get("flesch_kincaid_grade", 14)
    if 12 <= fk <= 16:
        return 1.0
    elif 10 <= fk < 12 or 16 < fk <= 18:
        return 0.7
    else:
        return 0.4


def _vocabulary_score(vocabulary):
    """èªå½™å“è³ªã‚’ 0-1 ã‚¹ã‚³ã‚¢ã«å¤‰æ›ã™ã‚‹ã€‚"""
    ttr = vocabulary.get("vocabulary_diversity_ttr", 0.5)
    redundant = len(vocabulary.get("redundant_phrases", []))
    overclaim = vocabulary.get("overclaim_count", 0)

    score = min(1.0, ttr * 2)  # TTR 0.5 ã§æº€ç‚¹
    score -= redundant * 0.02  # å†—é•·è¡¨ç¾ 1 ã¤ã«ã¤ã -0.02
    score -= overclaim * 0.05  # éå‰°ä¸»å¼µ 1 ã¤ã«ã¤ã -0.05
    return max(0.0, round(score, 2))
```

## References

### Output Files

| ãƒ•ã‚¡ã‚¤ãƒ« | å½¢å¼ | ç”Ÿæˆã‚¿ã‚¤ãƒŸãƒ³ã‚° |
|---|---|---|
| `manuscript/quality_report.json` | å“è³ªã‚¹ã‚³ã‚¢ã‚«ãƒ¼ãƒ‰ | ãƒã‚§ãƒƒã‚¯å®Œäº†æ™‚ |

### å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¸€è¦§

| Dimension | ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | é©æ­£ç¯„å›²ï¼ˆå­¦è¡“è«–æ–‡ï¼‰ |
|---|---|---|
| å¯èª­æ€§ | Flesch-Kincaid Grade | 12-16 |
| å¯èª­æ€§ | Gunning Fog Index | 12-18 |
| å¯èª­æ€§ | å¹³å‡æ–‡é•· | 15-25 èª |
| æ§‹é€  | IMRAD ãƒãƒ©ãƒ³ã‚¹ | å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 15-35% |
| èªå½™ | TTR (èªå½™å¤šæ§˜æ€§) | > 0.4 |
| èªå½™ | å†—é•·è¡¨ç¾ | < 5 ç¨®é¡ |
| é©åˆæ€§ | èªæ•°åˆ¶é™ | ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ä¾å­˜ |
| å†ç¾æ€§ | çµ±è¨ˆæ‰‹æ³•è¨˜è¼‰ | å¿…é ˆ |

### å‚ç…§ã‚¹ã‚­ãƒ«

| ã‚¹ã‚­ãƒ« | é€£æº |
|---|---|
| `scientific-academic-writing` | åŸç¨¿ `manuscript/manuscript.md` ã®å“è³ªã‚’è©•ä¾¡ |
| `scientific-critical-review` | ã‚»ãƒ«ãƒ•ãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœã¨å“è³ªã‚¹ã‚³ã‚¢ã®ç…§åˆ |
| `scientific-peer-review-response` | æ”¹è¨‚å¾Œã®å“è³ªæ”¹å–„ã‚’å®šé‡çš„ã«æ¤œè¨¼ |
| `scientific-revision-tracker` | æ”¹è¨‚å‰å¾Œã®å“è³ªã‚¹ã‚³ã‚¢æ¯”è¼ƒ |
| `scientific-latex-formatter` | æŠ•ç¨¿å‰ã®æœ€çµ‚å“è³ªã‚²ãƒ¼ãƒˆ |
